{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import grad\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
       "h1, h2, h3 { color: #34495e; }\n",
       "p { line-height: 1.6; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
    "h1, h2, h3 { color: #34495e; }\n",
    "p { line-height: 1.6; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 3, hidden_dim = 3, num_hidden_layers = 2):\n",
    "\n",
    "        super(OPNN, self).__init__()\n",
    "\n",
    "        # input layers\n",
    "        layers = [\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Softmax()\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # hidden layers \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers) # unbounded/continuous\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: tanh is smoother than ReLU and works better for PDE tasks (that's why no GELU)\n",
    "final layer doesn't have activation, allows output range to be unconstrained\n",
    "requires_grad for auto grad on the PDE residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S: Spot Price\n",
    "K: Strike Price\n",
    "T: Time to Maturity\n",
    "\n",
    "Deeper Networks will overfit/struggle with PDE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0379]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# default 3, 3, 2\n",
    "model = OPNN()\n",
    "\n",
    "# sample batch S = 100, K = 100, T = 0.5\n",
    "# output.shape = [1, 1]\n",
    "sample_input = torch.tensor([[100.0, 100.0, 0.5]], requires_grad=True) # shape [1, 3]\n",
    "normalized_input = sample_input / torch.tensor([100.0, 100.0, 1.0]) # normalization\n",
    "\n",
    "output = model(normalized_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0004, -0.0008, -0.0911]], grad_fn=<DivBackward0>)\n",
      "tensor([2.4495e-08], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(normalized_input)\n",
    "\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.ones_like.html\n",
    "\n",
    "# calc first derivatives\n",
    "grad_output = torch.ones_like(output)\n",
    "dC_dinput = grad(output, sample_input, create_graph=True)[0]\n",
    "print(dC_dinput)\n",
    "\n",
    "# calc second derivatives\n",
    "dC_dS = dC_dinput[:, 0]\n",
    "d2C_dS2 = grad(dC_dS, sample_input, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0][:, 0]\n",
    "print(d2C_dS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 28\n"
     ]
    }
   ],
   "source": [
    "# weight test\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight: mean=-0.0771, std=0.3553\n",
      "model.0.bias: mean=0.2223, std=0.2306\n",
      "model.2.weight: mean=0.0153, std=0.3365\n",
      "model.2.bias: mean=0.4348, std=0.1111\n",
      "model.4.weight: mean=-0.0871, std=0.2986\n",
      "model.4.bias: mean=0.0611, std=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/z9p2nx8j4h94st3b3b450cnw0000gn/T/ipykernel_88412/4232487677.py:5: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")\n"
     ]
    }
   ],
   "source": [
    "# checking weights\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonicity test: tensor([0.0299, 0.0339, 0.0379, 0.0419], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# symmetry/invariance check\n",
    "\n",
    "# call prices should INCREASE with S, and convex\n",
    "\n",
    "\n",
    "\n",
    "S_values = torch.tensor([[80.0, 100.0, 0.5],\n",
    "                         [90.0, 100.0, 0.5],\n",
    "                         [100.0, 100.0, 0.5],\n",
    "                         [110.0, 100.0, 0.5]], requires_grad=True)\n",
    "\n",
    "S_values_norm = S_values / torch.tensor([100.0, 100.0, 1.0])\n",
    "prices = model(S_values_norm)\n",
    "print(\"Monotonicity test:\", prices.squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output DECREASES as S increases which is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes_call_price(S, K, T, r, sigma):\n",
    "    \"\"\"Vectorized Black-Scholes formula for call options\"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def generate_black_scholes_dataset(n_samples=10000, r=0.05, sigma=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Sample inputs: S, K, T\n",
    "    S = np.random.uniform(50, 150, size=n_samples)\n",
    "    K = np.random.uniform(50, 150, size=n_samples)\n",
    "    T = np.random.uniform(0.01, 1.0, size=n_samples)\n",
    "\n",
    "    # Compute prices using BS formula\n",
    "    C = black_scholes_call_price(S, K, T, r, sigma)\n",
    "\n",
    "    # Stack inputs and outputs\n",
    "    X = np.stack([S, K, T], axis=1)\n",
    "    y = C.reshape(-1, 1)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input (S, K, T): tensor([[ 87.4540,  87.3641,   0.7327],\n",
      "        [145.0714,  83.2912,   0.1927],\n",
      "        [123.1994,  67.6154,   0.3532]])\n",
      "Sample target prices (C): tensor([[ 7.6124],\n",
      "        [62.5787],\n",
      "        [56.7675]])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_black_scholes_dataset(n_samples=10000)\n",
    "\n",
    "print(\"Sample input (S, K, T):\", X_train[:3])\n",
    "print(\"Sample target prices (C):\", y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the simulator did well, within market range \\\n",
    "\n",
    "Sample input (S, K, T): \\\n",
    "tensor([[ 87.4540,  87.3641,  0.7327],  # near-the-money, moderate T \\\n",
    "        [145.0714,  83.2912,  0.1927],  # deep in-the-money \\\n",
    "        [123.1994,  67.6154,  0.3532]]) # deep in-the-money, mid-maturity \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample target prices (C): \\\n",
    "tensor([[ 7.6124],   # makes sense for near-the-money \\\n",
    "        [62.5787],  # very high because S >> K \\ \n",
    "        [56.7675]]) # similar case: S >> K \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared pde residual (physics informed)\n",
    "\n",
    "def bs_pde_loss(model, inputs, r=0.05, sigma=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Black-Scholes PDE residual loss.\n",
    "\n",
    "    inputs: tensor of shape [B, 3] = [S, K, T]\n",
    "    \"\"\"\n",
    "    inputs = inputs.clone().detach().requires_grad_(True)\n",
    "    S = inputs[:, 0:1]\n",
    "    K = inputs[:, 1:2]\n",
    "    T = inputs[:, 2:3]\n",
    "\n",
    "    \n",
    "    norm_inputs = inputs / torch.tensor([100.0, 100.0, 1.0])\n",
    "\n",
    "    C = model(norm_inputs)\n",
    "\n",
    "    # calculating the first derivative loss\n",
    "\n",
    "    dC = grad(C, inputs, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    dC_dS = dC[:, 0:1]\n",
    "    dC_dT = dC[:, 2:3]\n",
    "\n",
    "\n",
    "    # second derivative loss\n",
    "    d2C_dS2 = grad(dC_dS, inputs, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0][:, 0:1]\n",
    "\n",
    "    # linear transform of the \n",
    "    pde_residual = dC_dT + 0.5 * sigma**2 * S**2 * d2C_dS2 + r * S * dC_dS - r * C\n",
    "\n",
    "    # mse of the residual\n",
    "    loss = torch.mean(pde_residual**2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pde loss: 0.008035659790039062\n"
     ]
    }
   ],
   "source": [
    "# sample input batch, untrained models should give non-zero pde residuals\n",
    "\n",
    "inputs, _ = generate_black_scholes_dataset(n_samples=32)\n",
    "pde_loss = bs_pde_loss(model, inputs)\n",
    "print(\"pde loss:\", pde_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised model loss, standard\n",
    "\n",
    "# MSE between predicted and BS price\n",
    "\n",
    "def supervised_loss(model, inputs, targets):\n",
    "\n",
    "    norm_inputs = inputs / torch.tensor([100.0, 100.0, 1.0])\n",
    "\n",
    "    preds = model(norm_inputs)\n",
    "\n",
    "    return torch.nn.functional.mse_loss(preds, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(model, inputs, targets, lambda_sup=1.0, lambda_pde=1.0, r=0.05, sigma=0.2):\n",
    "    sup_loss = supervised_loss(model, inputs, targets)\n",
    "    pde_loss = bs_pde_loss(model, inputs, r=r, sigma=sigma)\n",
    "    return lambda_sup * sup_loss + lambda_pde * pde_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m total_loss(model, X_train_batch, y_train_batch,\n\u001b[1;32m      7\u001b[0m                       lambda_sup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, lambda_pde\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 500 # for now, should go to \n",
    "\n",
    "# in theory, pde constraint often slows convergence\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = total_loss(model, X_train_batch, y_train_batch,\n",
    "                      lambda_sup=1.0, lambda_pde=1.0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
