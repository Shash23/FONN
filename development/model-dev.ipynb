{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
       "h1, h2, h3 { color: #34495e; }\n",
       "p { line-height: 1.6; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
    "h1, h2, h3 { color: #34495e; }\n",
    "p { line-height: 1.6; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 3, hidden_dim = 3, num_hidden_layers = 2):\n",
    "\n",
    "        super(OPNN, self).__init__()\n",
    "\n",
    "        # input layers\n",
    "        layers = [\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # hidden layers \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers) # unbounded/continuous\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: tanh is smoother than ReLU and works better for PDE tasks (that's why no GELU)\n",
    "final layer doesn't have activation, allows output range to be unconstrained\n",
    "requires_grad for auto grad on the PDE residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S: Spot Price\n",
    "K: Strike Price\n",
    "T: Time to Maturity\n",
    "\n",
    "Deeper Networks will overfit/struggle with PDE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1164]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# default 3, 3, 2\n",
    "model = OPNN()\n",
    "\n",
    "# sample batch S = 100, K = 100, T = 0.5\n",
    "# output.shape = [1, 1]\n",
    "sample_input = torch.tensor([[100.0, 100.0, 0.5]], requires_grad=True) # shape [1, 3]\n",
    "normalized_input = sample_input / torch.tensor([100.0, 100.0, 1.0]) # normalization\n",
    "\n",
    "output = model(normalized_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1156e-04,  2.4392e-05,  1.5128e-02]], grad_fn=<DivBackward0>)\n",
      "tensor([1.8571e-07], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(normalized_input)\n",
    "\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.ones_like.html\n",
    "\n",
    "# calc first derivatives\n",
    "grad_output = torch.ones_like(output)\n",
    "dC_dinput = grad(output, sample_input, create_graph=True)[0]\n",
    "print(dC_dinput)\n",
    "\n",
    "# calc second derivatives\n",
    "dC_dS = dC_dinput[:, 0]\n",
    "d2C_dS2 = grad(dC_dS, sample_input, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0][:, 0]\n",
    "print(d2C_dS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 28\n"
     ]
    }
   ],
   "source": [
    "# weight test\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight: mean=0.0094, std=0.3078\n",
      "model.0.bias: mean=-0.0264, std=0.5426\n",
      "model.2.weight: mean=-0.0187, std=0.3512\n",
      "model.2.bias: mean=-0.1440, std=0.2526\n",
      "model.4.weight: mean=0.1386, std=0.3430\n",
      "model.4.bias: mean=0.3073, std=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/z9p2nx8j4h94st3b3b450cnw0000gn/T/ipykernel_62290/4232487677.py:5: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")\n"
     ]
    }
   ],
   "source": [
    "# checking weights\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monotonicity test: tensor([0.1187, 0.1175, 0.1164, 0.1153], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# symmetry/invariance check\n",
    "\n",
    "# call prices should INCREASE with S, and convex\n",
    "\n",
    "\n",
    "\n",
    "S_values = torch.tensor([[80.0, 100.0, 0.5],\n",
    "                         [90.0, 100.0, 0.5],\n",
    "                         [100.0, 100.0, 0.5],\n",
    "                         [110.0, 100.0, 0.5]], requires_grad=True)\n",
    "\n",
    "S_values_norm = S_values / torch.tensor([100.0, 100.0, 1.0])\n",
    "prices = model(S_values_norm)\n",
    "print(\"Monotonicity test:\", prices.squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output DECREASES as S increases which is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes_call_price(S, K, T, r, sigma):\n",
    "    \"\"\"Vectorized Black-Scholes formula for call options\"\"\"\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def generate_black_scholes_dataset(n_samples=10000, r=0.05, sigma=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Sample inputs: S, K, T\n",
    "    S = np.random.uniform(50, 150, size=n_samples)\n",
    "    K = np.random.uniform(50, 150, size=n_samples)\n",
    "    T = np.random.uniform(0.01, 1.0, size=n_samples)\n",
    "\n",
    "    # Compute prices using BS formula\n",
    "    C = black_scholes_call_price(S, K, T, r, sigma)\n",
    "\n",
    "    # Stack inputs and outputs\n",
    "    X = np.stack([S, K, T], axis=1)\n",
    "    y = C.reshape(-1, 1)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input (S, K, T): tensor([[ 87.4540,  87.3641,   0.7327],\n",
      "        [145.0714,  83.2912,   0.1927],\n",
      "        [123.1994,  67.6154,   0.3532]])\n",
      "Sample target prices (C): tensor([[ 7.6124],\n",
      "        [62.5787],\n",
      "        [56.7675]])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_black_scholes_dataset(n_samples=10000)\n",
    "\n",
    "print(\"Sample input (S, K, T):\", X_train[:3])\n",
    "print(\"Sample target prices (C):\", y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the simulator did well, within market range \\\n",
    "\n",
    "Sample input (S, K, T): \\\n",
    "tensor([[ 87.4540,  87.3641,  0.7327],  # near-the-money, moderate T \\\n",
    "        [145.0714,  83.2912,  0.1927],  # deep in-the-money \\\n",
    "        [123.1994,  67.6154,  0.3532]]) # deep in-the-money, mid-maturity \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample target prices (C): \\\n",
    "tensor([[ 7.6124],   # makes sense for near-the-money \\\n",
    "        [62.5787],  # very high because S >> K \\ \n",
    "        [56.7675]]) # similar case: S >> K \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
