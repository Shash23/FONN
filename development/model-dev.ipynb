{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
       "h1, h2, h3 { color: #34495e; }\n",
       "p { line-height: 1.6; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "body { font-family: \"Helvetica Neue\", sans-serif; font-size: 15px; }\n",
    "h1, h2, h3 { color: #34495e; }\n",
    "p { line-height: 1.6; }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPNN(nn.Module):\n",
    "\n",
    "    # x = [S/K, T]\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 50, num_hidden_layers = 2):\n",
    "\n",
    "        super(OPNN, self).__init__()\n",
    "\n",
    "        # input layers\n",
    "        layers = [\n",
    "            \n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # hidden layers \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers) # unbounded/continuous\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # https://stats.stackexchange.com/questions/501437/why-are-the-softmax-softplus-and-softsign-functions-all-prefixed-with-the-word\n",
    "\n",
    "        # adapted from relu but smoother and differentiable everywhere\n",
    "        # ln(1 + e^x)\n",
    "\n",
    "        # https://docs.pytorch.org/docs/stable/generated/torch.nn.Softplus.html\n",
    "        return F.softplus(self.model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: tanh is smoother than ReLU and works better for PDE tasks (that's why no GELU)\n",
    "final layer doesn't have activation, allows output range to be unconstrained\n",
    "requires_grad for auto grad on the PDE residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S: Spot Price\n",
    "K: Strike Price\n",
    "T: Time to Maturity\n",
    "\n",
    "Deeper Networks will overfit/struggle with PDE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = OPNN(input_dim=2, hidden_dim=50, num_hidden_layers=2)\\n\\n# Sample batch: S = 100, K = 100, T = 0.5\\n# Output.shape = [1, 1]\\nsample_S = torch.tensor([[100.0]], requires_grad=True)\\nsample_K = torch.tensor([[100.0]], requires_grad=True)\\nsample_T = torch.tensor([[0.5]], requires_grad=True)\\n\\n\\nnn_input = torch.cat([sample_S / sample_K, sample_T], dim=1)\\n\\noutput_normalized = model(nn_input)\\noutput_C = output_normalized * sample_K\\n\\nprint(\"Model output (normalized C/K):\", output_normalized)\\nprint(\"Model output (un-normalized C):\", output_C)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default 3, 3, 2\n",
    "\n",
    "'''\n",
    "model = OPNN(input_dim=2, hidden_dim=50, num_hidden_layers=2)\n",
    "\n",
    "# Sample batch: S = 100, K = 100, T = 0.5\n",
    "# Output.shape = [1, 1]\n",
    "sample_S = torch.tensor([[100.0]], requires_grad=True)\n",
    "sample_K = torch.tensor([[100.0]], requires_grad=True)\n",
    "sample_T = torch.tensor([[0.5]], requires_grad=True)\n",
    "\n",
    "\n",
    "nn_input = torch.cat([sample_S / sample_K, sample_T], dim=1)\n",
    "\n",
    "output_normalized = model(nn_input)\n",
    "output_C = output_normalized * sample_K\n",
    "\n",
    "print(\"Model output (normalized C/K):\", output_normalized)\n",
    "print(\"Model output (un-normalized C):\", output_C)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noutput = model(normalized_input)\\n\\n# https://docs.pytorch.org/docs/stable/generated/torch.ones_like.html\\n\\n# calc first derivatives\\ngrad_output = torch.ones_like(output)\\ndC_dinput = grad(output, sample_input, create_graph=True)[0]\\nprint(dC_dinput)\\n\\n# calc second derivatives\\ndC_dS = dC_dinput[:, 0]\\nd2C_dS2 = grad(dC_dS, sample_input, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0][:, 0]\\nprint(d2C_dS2)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "output = model(normalized_input)\n",
    "\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.ones_like.html\n",
    "\n",
    "# calc first derivatives\n",
    "grad_output = torch.ones_like(output)\n",
    "dC_dinput = grad(output, sample_input, create_graph=True)[0]\n",
    "print(dC_dinput)\n",
    "\n",
    "# calc second derivatives\n",
    "dC_dS = dC_dinput[:, 0]\n",
    "d2C_dS2 = grad(dC_dS, sample_input, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0][:, 0]\n",
    "print(d2C_dS2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 2751\n"
     ]
    }
   ],
   "source": [
    "# weight test\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight: mean=-0.0123, std=0.4252\n",
      "model.0.bias: mean=-0.0293, std=0.4250\n",
      "model.2.weight: mean=0.0020, std=0.0806\n",
      "model.2.bias: mean=0.0235, std=0.0727\n",
      "model.4.weight: mean=-0.0088, std=0.0753\n",
      "model.4.bias: mean=0.1073, std=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/z9p2nx8j4h94st3b3b450cnw0000gn/T/ipykernel_84272/4232487677.py:5: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")\n"
     ]
    }
   ],
   "source": [
    "# checking weights\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nS_values = torch.tensor([[80.0, 100.0, 0.5],\\n                         [90.0, 100.0, 0.5],\\n                         [100.0, 100.0, 0.5],\\n                         [110.0, 100.0, 0.5]], requires_grad=True)\\n\\nS_values_norm = S_values / torch.tensor([100.0, 100.0, 1.0])\\nprices = model(S_values_norm)\\nprint(\"Monotonicity test:\", prices.squeeze())\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symmetry/invariance check\n",
    "\n",
    "# call prices should INCREASE with S, and convex\n",
    "\n",
    "\n",
    "'''\n",
    "S_values = torch.tensor([[80.0, 100.0, 0.5],\n",
    "                         [90.0, 100.0, 0.5],\n",
    "                         [100.0, 100.0, 0.5],\n",
    "                         [110.0, 100.0, 0.5]], requires_grad=True)\n",
    "\n",
    "S_values_norm = S_values / torch.tensor([100.0, 100.0, 1.0])\n",
    "prices = model(S_values_norm)\n",
    "print(\"Monotonicity test:\", prices.squeeze())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output DECREASES as S increases which is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vectorized black scholes for call options\n",
    "\n",
    "small epsilon to avoid distribution by zero or log of zero for T if T can be close to 0\n",
    "for now, lets assume T > 0 is based on uniform(0.001, 1.0)\n",
    "'''\n",
    "def black_scholes_call_price(S, K, T, r, sigma):\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n",
    "    \n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    \n",
    "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "\n",
    "def generate_black_scholes_dataset(n_samples=10000, r=0.05, sigma=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Sample inputs: S, K, T\n",
    "    S = np.random.uniform(5, 200, size=n_samples) # Example range, adjust as needed\n",
    "    K = np.random.uniform(5, 200, size=n_samples) # Example range, adjust as needed\n",
    "    T = np.random.uniform(0.01, 1.0, size=n_samples) # Time to Maturity\n",
    "\n",
    "    # Compute the true Black-Scholes call option price\n",
    "    C = black_scholes_call_price(S, K, T, r, sigma)\n",
    "\n",
    "    # Combine S, K, T into a single input tensor (raw_inputs)\n",
    "    # This `raw_inputs` tensor will be passed to `total_loss`, `supervised_loss`, and `bs_pde_loss`\n",
    "    raw_inputs = np.stack([S, K, T], axis=1) # Shape [n_samples, 3]\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    y_tensor = torch.tensor(C.reshape(-1, 1), dtype=torch.float32) # Target prices (C)\n",
    "    X_tensor = torch.tensor(raw_inputs, dtype=torch.float32) # Raw inputs [S, K, T]\n",
    "\n",
    "    return X_tensor, y_tensor # X_tensor now contains [S, K, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input (S, K, T): tensor([[ 78.0353,  77.8600,   0.7327],\n",
      "        [190.3893,  69.9179,   0.1927],\n",
      "        [147.7388,  39.3500,   0.3532]])\n",
      "Sample target prices (C): tensor([[  6.8434],\n",
      "        [121.1417],\n",
      "        [109.0776]])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = generate_black_scholes_dataset(n_samples=10000)\n",
    "\n",
    "print(\"Sample input (S, K, T):\", X_train[:3])\n",
    "print(\"Sample target prices (C):\", y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the simulator did well, within market range \\\n",
    "\n",
    "Sample input (S, K, T): \\\n",
    "tensor([[ 87.4540,  87.3641,  0.7327],  # near-the-money, moderate T \\\n",
    "        [145.0714,  83.2912,  0.1927],  # deep in-the-money \\\n",
    "        [123.1994,  67.6154,  0.3532]]) # deep in-the-money, mid-maturity \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample target prices (C): \\\n",
    "tensor([[ 7.6124],   # makes sense for near-the-money \\\n",
    "        [62.5787],  # very high because S >> K \\ \n",
    "        [56.7675]]) # similar case: S >> K \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared pde residual (physics informed)\n",
    "'''\n",
    "Computes the Black-Scholes PDE residual loss.\n",
    "The PDE is: dC/dT + 0.5 * sigma^2 * S^2 * d2C/dS2 + r * S * dC/dS - r * C = 0 \n",
    "inputs: tensor of shape [B, 3] = [S, K, T] (raw S, K, T values for PDE calculations)\n",
    "'''\n",
    "def bs_pde_loss(model, inputs, r=0.05, sigma=0.2):\n",
    "    \"\"\"\n",
    "    Computes the Black-Scholes PDE residual loss.\n",
    "\n",
    "    inputs: tensor of shape [B, 3] = [S, K, T]\n",
    "    \"\"\"\n",
    "    # Ensure inputs require gradients for derivative computations\n",
    "    # IMPORTANT: All subsequent slices for S, K, T must come from this tensor\n",
    "    inputs_for_grad = inputs.clone().detach().requires_grad_(True)\n",
    "\n",
    "    S = inputs_for_grad[:, 0:1] # Slice from the tensor that requires grad\n",
    "    K = inputs_for_grad[:, 1:2] # Slice from the tensor that requires grad\n",
    "    T = inputs_for_grad[:, 2:3] # Slice from the tensor that requires grad\n",
    "\n",
    "    # ... rest of your code is fine from here onwards ...\n",
    "\n",
    "    price_ratio = S / K\n",
    "    nn_inputs = torch.cat([price_ratio, T], dim=1)\n",
    "\n",
    "    C_normalized = model(nn_inputs)\n",
    "\n",
    "    C = C_normalized * K\n",
    "\n",
    "    # calc the dervis for pde\n",
    "    dC_dT = grad(C, T, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    dC_dS = grad(C, S, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
    "    d2C_dS2 = grad(dC_dS, S, grad_outputs=torch.ones_like(dC_dS), create_graph=True)[0]\n",
    "\n",
    "    # total pde is: dC/dT + 0.5 * sigma^2 * S^2 * d2C/dS2 + r * S * dC/dS - r * C = 0\n",
    "    pde_residual = dC_dT + 0.5 * sigma**2 * S**2 * d2C_dS2 + r * S * dC_dS - r * C\n",
    "\n",
    "    loss = torch.mean(pde_residual**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pde loss: 14.142163276672363\n"
     ]
    }
   ],
   "source": [
    "# sample input batch, untrained models should give non-zero pde residuals\n",
    "\n",
    "inputs, _ = generate_black_scholes_dataset(n_samples=32)\n",
    "pde_loss = bs_pde_loss(model, inputs)\n",
    "print(\"pde loss:\", pde_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised model loss, standard\n",
    "\n",
    "# MSE between predicted and BS price\n",
    "\n",
    "def supervised_loss(model, inputs, targets):\n",
    "\n",
    "    S = inputs[:, 0:1]\n",
    "    K = inputs[:, 1:2]\n",
    "    T = inputs[:, 2:3]\n",
    "\n",
    "    price_ratio = S / K\n",
    "    nn_input_features = torch.cat([price_ratio, T], dim=1)\n",
    "\n",
    "    predicted_C_normalized = model(nn_input_features)\n",
    "\n",
    "    predicted_C = predicted_C_normalized * K\n",
    "\n",
    "    return F.mse_loss(predicted_C, targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "combine the supervised loss and pde loss\n",
    "\n",
    "next steps: dynamically weigh the lambdas to find optimal model\n",
    "\n",
    "'''\n",
    "def total_loss(model, inputs, targets, lambda_sup=1.0, lambda_pde=1.0, r=0.05, sigma=0.2):\n",
    "    \n",
    "    sup_loss = supervised_loss(model, inputs, targets)\n",
    "    \n",
    "    pde_loss = bs_pde_loss(model, inputs, r=r, sigma=sigma)\n",
    "    \n",
    "    return lambda_sup * sup_loss + lambda_pde * pde_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 250 epochs...\n",
      "Epoch [1/250], Loss: 1663.587832\n",
      "Epoch [10/250], Loss: 45.044433\n",
      "Epoch [20/250], Loss: 16.830009\n",
      "Epoch [30/250], Loss: 11.038599\n",
      "Epoch [40/250], Loss: 9.417383\n",
      "Epoch [50/250], Loss: 8.448577\n",
      "Epoch [60/250], Loss: 8.152265\n",
      "Epoch [70/250], Loss: 7.695060\n",
      "Epoch [80/250], Loss: 7.383376\n",
      "Epoch [90/250], Loss: 7.402732\n",
      "Epoch [100/250], Loss: 7.516372\n",
      "Epoch [110/250], Loss: 7.286956\n",
      "Epoch [120/250], Loss: 7.288943\n",
      "Epoch [130/250], Loss: 7.500748\n",
      "Epoch [140/250], Loss: 7.165808\n",
      "Epoch [150/250], Loss: 7.106995\n",
      "Epoch [160/250], Loss: 7.133197\n",
      "Epoch [170/250], Loss: 6.967560\n",
      "Epoch [180/250], Loss: 7.145759\n",
      "Epoch [190/250], Loss: 7.257288\n",
      "Epoch [200/250], Loss: 7.216045\n",
      "Epoch [210/250], Loss: 7.106313\n",
      "Epoch [220/250], Loss: 7.347358\n",
      "Epoch [230/250], Loss: 6.968484\n",
      "Epoch [240/250], Loss: 7.035211\n",
      "Epoch [250/250], Loss: 7.010589\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate Training Data\n",
    "# X_train will contain [S, K, T] and y_train will contain [C]\n",
    "X_train, y_train = generate_black_scholes_dataset(n_samples=10000, r=0.05, sigma=0.2, seed=42)\n",
    "\n",
    "# Instantiate the Model\n",
    "model = OPNN(input_dim=2, hidden_dim=50, num_hidden_layers=2)\n",
    "\n",
    "# Choose an Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare Data Loaders for Batching\n",
    "batch_size = 64 # testing with various batch sizes\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define Training Parameters (Moved to before the loop)\n",
    "num_epochs = 250 # this is what's defined in the paper but lets see \n",
    "r = 0.05 # risk free from data gen\n",
    "sigma = 0.2 # same thing with the vol\n",
    "\n",
    "# The paper said that there are weights but didn't specify the exact values.\n",
    "# We are assuming equal weighting for now.\n",
    "lambda_supervised = 1.0\n",
    "lambda_pde = 1.0\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_idx, (inputs_batch, targets_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = total_loss(\n",
    "            model,\n",
    "            inputs_batch,\n",
    "            targets_batch,\n",
    "            lambda_sup=lambda_supervised,\n",
    "            lambda_pde=lambda_pde,\n",
    "            r=r,\n",
    "            sigma=sigma\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
